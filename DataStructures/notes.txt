==========
Linked In:
==========
Data Types:

Decimal:
  - Float - 32 bits ~7 decimal digits 
  - Double - 64 bits ~16 decimal digits
  
Whole Numbers:
  - short - 16 bits (-32768 to 32767)
  - int - 32 bits (-2billion to 2 billion)
  - long - 64 bits (-2~63 to 2~63)
  
Signed and Unsigned Numbers
  - Signed : +ve and -ve values
  - Unsigned : only +ve
  
Javascript has just numbers
Python: int long float and complex. Python 3 long(int)

Java : float x = 10.0f;
JavaScript: var myNum =29

Boolean : true/false
Python: isLightOn = true;

char
String - series of characters

Each data type (int, long , char, boolean , string etc) all store 0s or 1s abstracted by the programming languages

1 bit = 0/1
Data Structures : links individual data to form a data set

String in javascript:
var name = "Jessica"
inPython:
name = 'Jessica'; <-- ' or " or "'

Primitive data types - space consumed is always fixed irrespective of the value
Data Structures - depends

Referenced types : variable name --> memory address --> data  (Strings and data Structures)
Primitive data types : variable name --> data (int, long, char etc)

1. Arrays

- collection of items where each element is identified by index
- each element --> index and data. index ~ slot
- example movie hall seats
- index : 0,1,2,3,4.....,n
- Java : int[] lotteryNumbers = { 0, 2, 18, 40, 14, 30}; int first = lotteryNumbers[0];
- Python : lotteryNumbers = [0, 2, 18, 40, 14, 30 ]
           second = lotteryNumbers[1]
- accessing lotteryNumbers[6] which doesnt exist --> IndexArrayoutOfBounds
** jdoodle : online editor/compiler
- Arrays provide organization and storage
- movie hall seats is an example of multi dimensional array
- multi dimensional array : (0,0) ,(0,1) ,.... : 1st is row and 2nd is column
- multi dimensional array example in nodejs
var dinnerChoices [[ "salad" , "soup", "cheese"], ["chicken", "salmon", "lasagna"]]
- multi dimensional array is array of arrays
- jagged array : number of columns not fixed for each row
int[][] jagged = new int[3][];
jagged[0] = new int[2];
//set row 0
jagged[0][0] =8;
jagged[0][1] =10;

//set row 1
jagged[1] = new int[9];

//set row 3
jagged[2] = new int[4]{2,4,8,1,0}

- java and C++ : basic arrays cannot be resized
- Ruby, javascript : can be resized

- Resizable/dynamic/
- Java : Immutable : basic array data
- Mutable : Java classses gives us ArrayList 

arrayList - array under the surface
arraylist.add(2, 10)
if index is not added, it gets added at the end
add =push
remove=pop
ArrayList<Integer> myArrayList = new ArrayList<Integer>();
for(int i=0; i< 10: i++){
myArrayList.add(i);
}
myArrayList.add(2,10);
Sysout(myArrayList);
Sysout(myArrayList.indexOf(9));//-1 if the index doesnt exist
//this search is linear search behind teh scenes

Sorting:
number --> natural order ..asc or dec
strings/character ==> alphabetical
custom (class, object etc) ==> comparators

Big-O notation
Array - updation or retrival - O(1)
      - best case insertion - O(1)
      - worst case insertion - O(n) when array size is not enough
      - search - 1/n
      - deletion - 1/n
      - sorting - depends on algo
      
LinkedList, DoubleLinked List, Stack, Queue

Hash based:
Associative array : collection of key value
  - keys must be unique
  - order isnt imporatant
  - duplicate values are allowed
  - abstract data type
  - dictionaly, maps are exaples
  
  
Hashing:
  - one way (inputs --> hash) and not other way. u cannot get inputs from hash
  - e.g passwords can be stored as hash
  - Collision : 2 inputs producing same hash
  
HashTable:
  - implementation of associative abstract data type arry
  - always added as a key-value set
  
Tree:
  - root node, parent node, child node
  - siblings : having same parent node
  - leaf node : no child
  
Binary Tree:
  - max two child nodes for each parent
  
Binary Search tree:
  - sorted
  - left is less than parent and right is more than parent
  - can be unbalanced
  
Heap:
- always balanced
- root node is higest or lowest
- find min/max O(1)
- insert O(log(n)
- search O(n)
- delete O(n)

Priority queue
- dont care about insertion order
- heaps are used to implement this

Balanced - O(log(n))
Unbalanced - O(n)

==========
Raghavendra
==========

Euclids algorithm
- m and n are 2 +ve integers
- find gcd of these m and n where m > n
- divide m/ n and lets say remainder is r
- if r=0, n is the gcd
- else m=n, n=r and repeat till r=0
** try this in program



 
 Analysis:
 - space complexity (lessimp since memory is cheap nowadays)
 - Time complexity (more imp)
    - Worst case (consider this)
    - best case (dont consider this)
    - average case (complex)
    
 Assumptions:
 - infinite memory
 - each opearion takes unit time
 - each memory access, unit time consumed
 - data is in RAM
 
 Bubble sort analysis:
 Sorting:
- 10,2,8,6,7,3
- sort in ascending order
- rules:
  - pick only 1 ball at a time
  - start from left most and move rightwards
 - take first 2 elements and swap if they are not ascending/desecnding, then take next 2 and continue till u reach end
 - comparison is 1 unit
 - swapis 3 units time
 - total (3+1)* (n-1 * n-2 * ...1 = 4n(n-1)
 - pseudo code:
 for i = 0 --> Array.length-2
 for j = 0 --> Array.length-2-i
 if Array[j] > Array[j+1]
 tmp=Array[j+1]
 Array[j+1] = Array[j]
 Array[j]=tmp
 
 **try
 

 constant time <= logn <= sq root n <= n(linear) <= nlogn <= n*2 <= n*3 <= 2*n <= n!(exponential)
 
 Array access elements by index : Constant time : O(1)
 Search particular element where index not known:
  - Time complexity depends on no of elements
  - O(n) <---linear time
  
  Bubble Sort Time Complexity:
    - 2 loops one inside another
    - O(n*2) --> n-1 + n-2 + .....+1
    - other sort algos with n*2 : Selection sort and Insertion Sort
  
  Selection SOrt:
    - find the lowest number and swap with first and put a pointer to 1st element
    - then from pointer to end,find the lowest and swap with 2nd (pointer +1 )
    - repeat till end
    - pseudo code:
    for i=0 to A.length - 2 
      minIndex = i;
      for j =i+1 to A.lenth -1
        if data[j] < data [minIndex]
          minIndex = j;
      tmp = data[minIndex]
      data[minIndex] = data[i]
      data[i] = temp
   **try
      
  Insertion Sort: 
    - inspired by cards shuffling
    - cards in hand are always sorted
    - new card taken and put in the sorted list at the correct position by first comparing the last element in sorted list and recursively till end
    - psuedo code:
    for i = 0 to A.lenth -1
      current = A[i]
      j = i -1
      while j >= 0 && A[j] > current
        A[j+1] = A[j]
        j=j-1
      A[j+1] =current
   ** try
   
   - Bubble sort is most inefficient, never used
   - Selection sort is better tahn bubble sort. Running time is independent of ordering of elements
   - Insertion sort is versatile. Effecient for small lists. Can be used with other sorting algos.Relatively good for partially sorted lists
   
   Stable vs Unstable Sorts:
    - take for example an unsorted array : 3,5,2,1,5',10. There are 2 5's here- 5 and 5'
    - After sorting:
      - 1,2,3,5,5',10 <-- Stable sort since 5' still holds its place
      - 1,2,3,5',5,10 <-- Unstable sort since 5' and 5 are interchanged
    - while j >= 0 && A[j] > current in insertion sort results in stable sort
    - while j >= 0 && A[j] > current in insertion sort results in unstable sort
    - example say we have data as name and age and age is already sorted and we have 2 names same. After unstable sort sorting of age goes away unlike stable sort
    ***try stable and unstable scenarios
   
  SEARCHING:
  =========
  Array:
    - access element via index O(1)
    - find element where index not known O(n)
  Ordered Arrays:
  FIND:
    - find middle index --> (A.length - 1) + 0 / 2 --> e.g say m
    - check A[m] matches the number u are searching
    - if not check 0 --> m-1 array or m + 1 to A.length -1 and repeat
    - Time complexity
      - first search - n
      - second search n/2
      - third n/3
      ....
      - i-1 th search is 2
      - i th search is 1 
      - find complexity is O(log2 n)
   INSERTION:
      - where to insert? as per order
      - create space --> shift all > the number to be inserted and insert
      - O(n)
   Delete: Delete number and move --> O(n)
  
    
LinkedList
===========
- Nodes with link to next
- First node is HEAD
- Node = data + link to next Node
- If last node, then link to next node is null
- only reference we have ina linked list is that of head
**try
Class Node:
private int data;
private Node nextNode;
//getters and setters for data and nextNode
//constructor
public Node(int data) {
this.data = data;
}
Class LinkedList:
private Node head;
//other methods
//Insertion: add new element as head
- create new node
- point new node to head
- make head point to new node
- this sequence is imp if we move head as step 2 then linkedlist will be gone as there is no head
- O(1) beause we always add at head and no travelsal needed
//Deletion
- just point head to head.next
- though the orig head node exists and points to current head, nothing points to it and hence orig head is garbage collected
-O(1)
//Searching
- traverse thru all nodes
- linear time operation
- O(n)
Doubly Ended Linked List:
- Linkedlist having head and tail
- Insertion at tail

Empty Linked list:
- head points to null

Sorted Linked List:
- Insertion:
  - traverse one by one and find the node whose data is more than given node to be inserted. e.g say u traversed and found 
  nth node > given nodeto be inserted. Now u can point givennode.next to nth node but u cant get the n-1th node (since u c    annot   traverse backwards) to point to given node.
  - right way: currentNode.nextNode.getData > givenNode.getData() 
  - O(n)
  
Doubly Linked List:
- Node = data + nextNode pointer + previousNode pointer
- head.prevNode = null
- Insert a new element : O(1)
**try

Insertion sort revisited:
- use Doubly Linked List instead of array
- even tho same time complexity,in reality faster since constanst are less due to less memory consumption
** try - imp

STACKS
======
- push : only can be added at the top
- pop : only can remove the top element of the stack
- peek : reads the value of the topmost element without removing it
- LIFO : Last In First Out
- stacks and queues are abstract data type unlike arrays or LinkedList (which are real data structures, which are physically represented in memory as such and can hold real data) But stacks and queues are conceptual data structures which are usually implemented using an array or LinkedList.
- stacks using array:
maxSize =9;
int[] stackArray = new int[maxSize];
int top=-1;// implies stack is empty
Implementing stacks using arrays : We know that arrays have a fixed length, so first we declare the maximum size of the array, say nine, and let's take an array of int type of size nine. Now we have an array and each element is initialized to zero. Now set the difference to keep track of the top of the stack and we initialize it to minus one because right now there are no elements in the stack. So let's say we want to insert or push a new element five. Think about how we can do that, well all we need to do is implement the value of top by one, so it points to the zero the next now, and we update the data at index zero to five, that's it. Now let's push 12, so we implement the top by one again to make it point to the first index and update the data at index one to twelve. Similarly we can keep pushing other elements like this. Now let's see the peek operation. All peek does here is return the element pointed by the top index, which in this case is two, and the value of top is not changed in case of the peek operation alright? Let's see the pop operation now. The pop operation as you know should remove the top most element, and how can we do that? Well it returns the element pointed by the top, which is two here, and decrements the value of top by one, which is essentially removing the top element, because we don't have a reference to that element any more, and the top now points to the element at index two, which is seven. Similarly we can remove the element seven by returning seven and then decrementing the value of top by one. Now if we remove all the elements of the stack, and the value of top is minus one again, we can say that the stack is empty. 
- All the three operations we can perform on a stock, push, pop, and peek are all constant time operations, that is all of these operations take all one time, why? Because the time taken by these operations don't really depend on the number of items that are in the stack. If you want to push a new item it goes right on top of the stack and we can access that through the top index value which is immediately available to us. Similarly if we have two peek or pop elements from a stack we again access the element using the top index which does not really depend on the size of the stack.
- all push pop and peek have constant time complexities O(1)

Queues:
- Queues, just like stacks, are also abstract data types. That is, they are implemented using some real concrete data structures like an array or a link list. 
- As the name suggests, the queue data structure basically is like a bunch of people standing in a queue at a theater to get in. So the first guy stands at the front, and then the next guy behind him, and so on. 
- So a queue can be thought of as a container whose front end is called the head, and the rear end is called the tail. And we can insert and remove data elements from a queue. 
- Insertion : You insert an element into the queue from the tail end and the element goes forward as much as it can towards the head like shown here. And this operation is called enqueue. So the enqueue operation inserts a new element from the tail and sort of pushes it towards the head of the queue. We can keep inserting elements like this. 
- Deletion : We remove elements one by one at the head of the queue, that is the first element at the head is pushed out of the queue first. So here the first element at the head, which is seven, is removed first. This operation of removing an element from the queue is called dequeue, and it always removes the element at the head of the queue. This time if we use the dequeue operation, the element 12 will be removed. Basically in queues, elements enter from the tail and leave from the head. If you remember in stacks, the last element to go in was the first one to come out. But in queues, the first element that goes in is the first element to come out. 
- So queues are also called fifo type of data structures; first in first out. 
- Peek : The peek operation returns the value of the element at the head without removing it. So here the peek operation will return 21. So there are three operations that can be performed on a queue; endqueue, dequeue, and peek. 
- And, of course, we can check the size of the queue, or check if the queue is full so that we can use it before endqueuing a new element.
- Queues using arrays
- [Instructor] We saw the operations we can perform on queues. Now let's try and understand how we can implement a queue using an array. First we create a new array of int type of some size, say eight. And all the elements of this array are initialized to zero. And that's done by Java automatically, so you would know. Since there are no data elements at this point, we can initialize the head and the tail of the queue to minus one. Let's say we want to enqueue a new data element it in the queue. He can do that incrementing the values of both the head and tail by one, because this is the first element in the queue, both the head and the tail should be pointing to the zeroth index. Then we update the data at the zeroth index of eight isn't it? After all if there is only one person standing in a queue, he can be said to be at the head of the queue and as well as at the tail of the queue right? Now to add another element 12. Remember in the last case, we incremented the values of both the head and tail by one, because it was the first element, but now we only need to increment the value of tail by one for each enqueue operation. So to enqueue 12, we increment the value of tail by one and update the data at the first index to 12. After all the first element remains at the head, because that has not moved out of the queue yet. Similarly we can keep enqueuing elements by incrementing the tail value of one. And updating the data. Now let's see how to dequeue elements? Now we always remove elements from the head of the queue, so for dequeuing we need to increment the value of head by one so that the head now points to the next element in the queue, which was added right after the one which is being removed from the queue. The dequeue operation will return the value of eight, which is the element at the head and the head now moves to the first index. As you can see, we don't need to physically remove the element from the array. We just move the head index. So that the element is logically removed from the queue. It's a similar approach that we followed in case of stacks alright? So if we dequeue again, it returns 12, which is the element at the head and the head now moves to the second index. Now tail will remain at it's index throughout the dequeue operation. The next operation that we look at is peek. The peak operation here will return 73 which is the element at the head without removing it. As you can see the dequeue is in a state such that the tail is at the seventh index, which is the maximum index of this array. So now if you want to enqueue a new element say 27, is it possible to do it? How can we increment the tail? Well as you can see, we do have some empty spaces in the queue. After all, when some elements have been removed from the queue, we get some space. And hence that space can be reused. So the first thing that we need to find out is if there are any empty slots for the new element to be enqueued? You should pause the video here and think about how you can find that our programmatically? Well we can find that out by subtracting the head index from the tail index. And if this value is less than the length of the array, which is eight, we can be sure that there is an empty place on the queue. So in this case, the tail is at the seventh index and the head is at the third index. So it is clear that there is in fact empty place in the queue. Now that we are sure that there is an empty place, next we need to figure out where the tail needs to be placed. So we can see that the empty place is at the beginning of the array. So whenever the tail is at the maximum index position and the head is not at the zeroth index, we bring the tail back to the zeroth index. And then we can update the data at the zeroth index to 27. Similarly, we can add another element just like before by incrementing the value of tail by one. And updating the data at that index. A queue in which we enqueue elements by bringing the tail back to the zeroth index, after it has reached the maximum index position is called a circular queue because the tail traverses the queue in a circle to come back to the position it started.

maxSize = 8 ;
int[] queueArray = new int[maxSize];
int head = -1;
int tail = -1;

enqueue(8) ==> this will make head = tail = 0 ; queueArray[0] = 8;
enqueue(12) ==> head = 0, tail =1, queuArray[1] =12;
enqueue makes tail ++


8,12,17,73,19,12,3,98

peek() --> 8
returns value of head

deque() ==> head = 1, tail =1;
deque makes head++
we dont need to explicitly remove the element.

for enqueu:
- first check if there is empty space in teh queue
  Math.bas(Tail index - head index) < Length of array ==> space present
- Circular queue

Double Ended Queue:
- [Instructor] We have seen what a queue is and how it functions, now we will see another type of queue called the double-ended queue, or the DE queue, which kind of sounds similar to the DQ operation, but it's not that. The DE queue data structure is a double-ended queue data structure. Before we get into the details of it, I want to tell you that this kind of a queue is generally not used in real life programming because of its complexity. So we will not be discussing it in depth, neither will I go into its implementation. But we will see an animated overview of its functioning and try to understand the basic idea behind it. A DE queue is different from a normal queue in the way that it allows us to add and remove elements from both the head and the tail, unlike a normal queue, where we can add elements only to the tail and remove them only from the head. So here we have an empty DE queue, with its head and tail. So we can insert an element from the right. So when we use the Insert Right operation, a new element is added to the queue at the tail like shown here. We can also insert an item from the left, so when we use the Insert Left operation, elements are added at the head of the queue, like shown. Basically, we can choose to add new elements to either the head of the tail of the queue. Similarly, we can delete elements too, either from the head or the tail. There will be four main operations in a DE queue: Insert Right or enqueue right, whatever you want to call it. Insert Left or enqueue left. Delete Right, and Delete Left, alright?

Double-ended queues using arrays
- [Instructor] Now let's see how we can implement our DE queue using an array. First we create a new array of int type, of some size, say eight. Then we initialize the head and tail with a value of minus one, since there are no elements in the array right now. Now let's say we want to add a new element, seven, from the right. As it is the first element, it doesn't really matter. We do this by incrementing the value of both head and tail by one, and updating the data at the zeroth index to seven. Now, when we say we want to insert it from the right, it means that we want to insert it at the tail. Whenever we want to insert a new element at the tail, we need to increment the value of tail by one, and then update the data at that index. So here, to insert the element 12 at right, we need to increment the value of tail by one, and then update the data at the tail index, which is the first index right now, to 12. Now let's say we want to add a new element, 14, from the left. When we say left, we really mean at the head, but as you can see, we have the head at the zeroth index, and the tail at the first index, so if you want to insert an element at the head, where does the head move to? As the head can never cross the tail, we cannot move it to the right. So whenever we add an element at the head, we need to decrement the value of head by one, but since the head is already at the zeroth index, we cannot decrement it right now. So what do we do? If you remember, in the case of a circular queue, the tail travels through the queue in a circle, so we can implement the same logic here, and make the head move to the last index, which is the seventh index. So now, the head moves to the last index, and we update the data at that index to 14. Now, what happens if we want to add an element, say nine, from the left, that is, at the head? Well, to do so, we can decrement the value of head by one, as discussed earlier. So the head now moves to the sixth index, and we update the data at this index to nine. So if you want to add an element from the right, we increment the value of the tail by one, and if we want to add an element from the left, we decrement the value of head by one. We can also delete items from our DE queue from the head and the tail. Say we want to delete the element from the left, that is the head. The element at the head is 15, so we can do that by simply incrementing the value of head by one, so that the head now moves from index five to index six, and that's it. As you can see, the head now points to data element nine, and 15 is essentially deleted. What if we want to delete the element from the right? Deleting an item from the right means deleting the element at the tail, which is 13 here. We can delete this item by just decrementing the value of tail by one, so that the tail now moves to the previous index. As you can see, the tail now points to the element 12, and so 13 can be considered to be deleted. So we can delete elements at both the ends like this. As I said earlier, DE queues are normally not used much, so we will not be getting into its implementation details. So we have learned about simple queues, circular queues, and the DE queues. Now there is another type of queue known as the priority queue, but priority queues are very easily implemented with the heap data structure, so we will not talk about this right now, as its implementation and understanding will need some knowledge of heaps. So we will discuss about priority queues in the chapter on heaps.

Recursion:
Introduction
- implementing the factorial method using recursion
- how the recursion call stack works
- implement the Tower of Hanoi
- implement the merge sort

- revisiting Euclid's algorithm
public int gcd (int a , int b) {
  if (b==0) return a;
  return gcd (b, a%b);
}

Now this is a recursive algorithm because the gcd method or function calls itself until it evaluates to some integer value, which is returned, and the integer value is evaluated when the parameter b passed to the gcd method is zero, and this is called the breaking condition, and it is very important to have a breaking condition in recursive method calls. Until b is zero, the gcd method will keep calling itself over and over, okay? So recursion is when a method calls itself from within the same method. 

- how we calculate the factorial of a method:factorial of n is n into n minus one into n minus two all the way up to one
public int factorial (int n) {
  int result =1;
  for(int i=1;i<=n;i++){
    result *= i;
  }
  return result;
}

So we can initialize a variable with a value of one, and then in a for loop, we keep multiplying integers from one to n to the result, and storing it back in the same variable. And finally, when we are done, we return the result, all right? So this way of calculating factorial is an iterative way, because we iterate through all the numbers from one to n. But again, the factorial of a number n is defined as this, and it can also be written as n into factorial of n minus one. We have just grouped a part from n minus one onwards, all right? So if we implement factorial recursively, we can write the factorial method like this. 

Now, let's implement the factorial method in Java, and run it to see if it works. 

public static int factorial(int n){
  return n*factorial(n-1);
}

This produces a stack overflow exception. So what went wrong? Well, we forgot to put the breaking condition, because of which, the factorial kept calling itself over and over, unless it ran kind of out of memory, and we will understand why that happens, but the important point is that you must put a breaking condition. Otherwise, you will get a runtime error. So we'll put the breaking condition, or the base condition as if n is zero, we return one, because factorial of zero is one, and factorial of negative numbers is not defined, so if you want to put another check, that is, if n is less than zero, we put another exception saying that factorial for that n is undefined. Anyways, so if we run the program again, after putting the base condition, we get the correct value of the factorial of this number. So while using recursion in a program, make sure that you always put a correct breaking or base condition.


public static int factorial(int n){
  if (n ==0 ) return 1;
  return n*factorial(n-1);
}

Now, if you compare this recursive program to the iterative one, you can see that the recursive way is much more concise and cleaner, so recursion has its pros and cons. The recursive way makes our problems very concise and easy to understand, but the iterative way performs better in terms of how fast it runs or how much memory is consumed. Also, recursion may cause stack overflow errors in case the depth of recursion is not small. But suppose you have the right program to solve the Rubik's cube, or to solve the Tower of Hanoi. This kind of programs are very difficult to implement iteratively, but then, recursion requires a little getting used to. 
**try out all the problems in the chapter exercise and think about simple algorithms, which may also be done recursively.

Understanding recursion
- [Instructor] Now let's try to understand how recursion works, and why would we get that stack overflow error in case you forgot the breaking condition, or if the depth of recursion is big. So let's use the factorial program to understand this. Now as you may know that a program's runtime usually creates a stack, which is called a method stack or a call stack, using which it keeps track of local variables or expression, evaluation etcetera. Now this call stacked in context of recursive methods is also called as recursion stack. It's one and the same thing, just a different name. Now let's say that the factorial method is called with the parameter five. Inside the factorial method, it checks if the parameter is zero, and it is not. So this method returns five into factorial of four. But factorial four itself is a method call, whose value is not yet known. So this expression five into factorial four, is stored in what is called as a stack frame and pushed into the recursion stack. And now the call is made to factorial four. So this method is called again with the parameter four. Then again, call to the factorial method returns four into factorial three, whose value is unknown. So this expression is stored in a stack frame and pushed down the recursion stack. This keeps on happening until a known value is returned. So now we know that factorial of zero is one, and this value is replaced in the stack frame, which has factorial zero as part of the expression to be evaluated. So this value is replaced in that stack frame to get the value of factorial one now. And we keep popping stack frames and replacing the calculated factorial values until all the stack frames are popped from the recursion stack. And the value of the expression of the last stack frame is the result of the recursive method call. And that is the reason why if you forget the breaking condition, you get stack overflow error because the method keeps pushing stack frames into the stack, never reaching the breaking condition and ultimately the stack is full and it overflows alright? Sometimes you may also get a stack overflow error even after having a breaking condition. And that would happen if the recursive call is too deep. For example, you call the method with a very large parameter value. That is, there is a whole bunch of stack frames to be pushed into the stack, before an answer can be arrived at.


Tail recursion
- [Instructor] In the last section we saw how a recursion called stack works. Now can there be a way to avoid stack overflow kind of errors? Think about it. Well there is another type of recursion called tail recursion, which if optimized for, can avoid stack overflow errors. So the kind of recursion that we just saw was head recursion. So we can write a program using tail recursion like this. We write a factorial method which takes an int type of parameter and further in detail we want to calculate the factorial of and under n type of parameter result, which holds the value of the previous calculation. So every time we calculate the factorial the result of the calculation is passed in the result parameter. You will see what I mean by that in a second. Then we write the breaking condition, where if n is equal to zero, we just return the result itself, because that should be holding the value of the factorial. Otherwise we would cause a recall the factorial method and every time we do so, we decrease the value of n by one. And past there is our test, a result into n. So that we keep moving towards the breaking condition. We usually overload the method to make it use easier. For example, we expose this raw method, many programmers might make a mistake initializing the value of result or they have to know the internal details of the method. We would rather like them to just all call factorial and pass the appropriate number, which in turn may call the other method. Note that this result parameter is called accumulator, because it accumulates the value of previous calculations, giving us a definite value every time. Now if you notice, using this type of recursion does not need stacks, because we have definite value throughout the method code, right? After all the stack beams are used to keep track of intermediate calculations. But here in tail calls, we are passing a definite value each time. But the compiler of the language should be able to identify this as tail recursion. And unfortunately Java is not able to do so. So even if you write this kind of code in Java, it will still make you operate recursion in stack and stacks beams and is not able to take advantage of tail recursion. But some functional programming languages like Haskell, ML, etc., are able to optimize for tail recursion. Hence it becomes quite useful there.

public int factorial (int n, int result) {// result is the ACCUMLATOR here
if (n==0) return result;
return factorial(n-1, n*result);
}
factorial(int n){
return factorial(n-1);
}

Tower of Hanoi
- [Instructor] To show the application of recursion, there is nothing better than the puzzle Tower of Hanoi. Tower of Hanoi is a very interesting puzzle. And many of you may already know about it. So there is a story that there is a place called Hanoi I think in Vietnam, where there are three towers and with about 100 disks. And amongst there, have to move all the disks from one tower to another tower by using certain rules. And it is said that by the time all the disks are moved from one tower to the other, the world will end. But the world has already ended many times now, and it's probably become quite boring to even talk about it. But anyways, so let's go over the rules of the puzzle. Now here we have three towers A, B, and C. And we have some disks in tower A, which are arranged in the increasing order of size from top to bottom. And what we are supposed to do is to move all the disks from tower A to tower C, using tower B as an intermediate such that at no point of time a bigger disk comes on top of a smaller one. And one more rule is that we can only move one disk at a time alright? So let's look at a few examples to make the idea clear. So let's say we just have one disk. So this is simple. We can just move disk one from the tower A to tower C directly and we are done. By the way, we have named the towers and numbered the disks so that we can track the movement and in our program when we write one, we will be printing out these messages. So if there are two disks, we number them as one and two, one being on top of two alright? And this order of numbering the disk is important for our algorithm. So in general, we will start numbering the topmost disk as one, which would also happen to be the disk with the smallest radius. And then keep increasing the numbers as we go down the stack of disks. In this case of two disks, it's also quite simple. We first move disk one from tower A to B. Then what do we do? Move disk two from tower A to C. And finally move disk one from tower B to C. This was also quite simple and something which everyone can figure out. Then what about if we have three disks, and you will notice that by adding more disks the complexity of the problem increases and slowly with few more disks, it will become much harder to write all the steps correctly. Anyways so before moving on you may want to pause the video here, and write the steps yourself. This one is also quite easy. So what we can do is that we move disk one to tower C from tower A. Then move disk two from A to B. Move disk one from C to B move disk three from A to C and now we would like to move both these disks one and two to tower C. But because we can only move one disk at a time, and without keeping the bigger disk on top of the smaller one at any point of time, we first need to move disk one from tower B to tower A. And then move disk two from tower B to C. And then finally disk one from A to C. And this was a little more intense than the first two cases. And as the number of disks increase, identifying the steps become increasingly complex. And actually it is so complex that it will really be hard to write a program, which moves these disks iteratively and this is where recursion comes to help and makes this problem quite easy to program. All you have to do is to focus a little and align your thoughts along the lines of recursion. So let's say we write a method move which takes four parameters. The first one is the number of disks that need to be moved and then the other three parameters can be of care or string types representing the towers. So the first one out of these three is tower from so we passed A here. The second is tower two. That is the disks are to be moved from tower A to tower C. And the third parameter is the intermediate tower where we pass B as argument. When we call the move method with these parameters, we mean that move and disks from tower A to tower C, using tower B as the intermediate. And first you should convince yourself that this is possible. That is we can move all these disks from A to C, using another tower. And if you're convinced let's proceed. So if you can do that it essentially means that we should be able to move these n minus one disks from tower A to tower B using C as an intermediate right? Think about it. Because the nth disk is the largest in radius, it can remain untouched when we are moving those other n minus one disks like that. So we should be able to move these n minus one disks from A to B using C. Which means that we can call the move method for n minus one disks from A to B using C isn't it? Notice that the order of arguments for from tower, to tower, and using tower has changed. Then we can move disk n from A to C, which is what we can print. And then we recursively move these n minus one disks from tower B to tower C, using tower A. So we can again call the move method recursively with this different set of parameters. And this whole recursive approach, just needs to be synced in a little. And to help with that, let's revisit that three disk example again. So here what we are seeing is that we want to move three disks from tower A to C, using tower B, which means that we can move two disks from tower A to B using C. And what we did earlier was that we first moved disk one from A to C. Then we moved disk two from A to B and then moved disk one from C to B. So essentially the state of the system right now is exactly as if we move the two disks from tower A to B using tower C. Which was done recursively, and we don't really care about the details. So we are done with this part. Now we move disk three from A to C. And finally we move these two disks from B to C using tower A. And if we see the details, that is what we end up doing. So as you can see thinking recursively makes this problem much easier to solve, but of course it needs a little practice and patience to be able to align your mind in that direction. And of course we need to put up breaking condition, but let's implement this program and we will talk about the breaking condition there. Before moving on to the next lecture I suggest that you try implementing the program yourself first. It's okay to try and fail but it's not okay not to have tried alright?

move(n,'A','C','B')
  - move(n-1, 'A','B','C')
  - print moving disc n from A to C
  - move(n-1,'B','C','A')
  
Tower of Hanoi: Implementation
- [Instructor] Before explaining the implementation, let me run the program to show you what exactly needs to be done, just in case you're not aware of this problem. So we are supposed to implement this move method, such that it takes these parameters, which was already discussed earlier. And we call this move method from the main method. Like here, we say, move two discs from tower A to tower C, using tower B, all right? And when we run this program, we get the exact sequence of the steps that are supposed to be executed to complete this task. So, as you can see, we first move disc one from A to B, then move disc 2 from A to C, and finally move disc 1 from B to C. Similarly, let's try to create the steps for three discs. So, if you have understood the problem, pause the video here, and implement it before moving on. So let's just start implementing the move method. As you can see, we just have these three lines of code here, which we have already seen earlier in the animation. So, what we are essentially doing is that we are just picking up the first ten minus one discs, and moving them from tower from, to the intermediate tower, using the to tower. Then, we move the only disc, which is the nth disc from tower from, to tower to, and then we recursively move that bunch of n minus one discs from the intermediate tower to the to tower, or the destination tower, using the from tower as an intermediate, all right? Just close your eyes and visualize what's happening. It's quite straightforward. Let's try to run the program after writing the main method and calling the move method from the main method, and as you can see, we get a stack overflow error, because we didn't put the breaking, or the terminating condition for this recursive call. And what would be the breaking condition? Think about it. Well, the breaking condition is when we only have one disc to be moved, right? And in that case, what do we print? Well, we simply move disc one from the from tower to the destination tower, and that's it, right? That is the case we started with when we were showing the animation in the previous video, right, when we only had one disc to be moved from tower A to tower C, and now, if you run the program, everything works fine. So if you look at the code, it's quite easy to implement the tower of Hanoi puzzle using recursion. You just need to get used to it, all right?
***try yourself first.

Tower of Hanoi: Implementation

public static void move(int n, char from, char to, char inter) {
		if(n == 1)
			System.out.println("Moving disc 1 from  "+ from + " to " + to);
		else {
			move(n-1, from,inter,to);
			System.out.println("Moving disc " +n+ " from " + from + " to " + to );
			move(n-1, inter, to, from);
		}
		
	}


- [Instructor] Before explaining the implementation, let me run the program to show you what exactly needs to be done, just in case you're not aware of this problem. So we are supposed to implement this move method, such that it takes these parameters, which was already discussed earlier. And we call this move method from the main method. Like here, we say, move two discs from tower A to tower C, using tower B, all right? And when we run this program, we get the exact sequence of the steps that are supposed to be executed to complete this task. So, as you can see, we first move disc one from A to B, then move disc 2 from A to C, and finally move disc 1 from B to C. Similarly, let's try to create the steps for three discs. So, if you have understood the problem, pause the video here, and implement it before moving on. So let's just start implementing the move method. As you can see, we just have these three lines of code here, which we have already seen earlier in the animation. So, what we are essentially doing is that we are just picking up the first ten minus one discs, and moving them from tower from, to the intermediate tower, using the to tower. Then, we move the only disc, which is the nth disc from tower from, to tower to, and then we recursively move that bunch of n minus one discs from the intermediate tower to the to tower, or the destination tower, using the from tower as an intermediate, all right? Just close your eyes and visualize what's happening. It's quite straightforward. Let's try to run the program after writing the main method and calling the move method from the main method, and as you can see, we get a stack overflow error, because we didn't put the breaking, or the terminating condition for this recursive call. And what would be the breaking condition? Think about it. Well, the breaking condition is when we only have one disc to be moved, right? And in that case, what do we print? Well, we simply move disc one from the from tower to the destination tower, and that's it, right? That is the case we started with when we were showing the animation in the previous video, right, when we only had one disc to be moved from tower A to tower C, and now, if you run the program, everything works fine. So if you look at the code, it's quite easy to implement the tower of Hanoi puzzle using recursion. You just need to get used to it, all right?


Merge sort
- This algorithm is a recursive algorithm and works very well for large sets of data. The most important step in the merge sort algorithm is the merge step. In this step, we take two individually sorted arrays, all right, and merge these two such that the resulting array is also sorted. So if the two arrays that we start with are in ascending order, the resulting array, which would contain all the elements of the two arrays, should also be sorted.

- Let's understand with the help of an example. We have these two arrays which are individually sorted, right.
2,9     7,8,12
2,7,8,9,12

Elements two and nine are in ascending order in the left array, while elements seven, eight, and 12 in the right array are also sorted. And if we merge them into one single array, so let's say we have a resulting array of size five, which is equal to the sum of the individual lengths of the two arrays, and the way to merge these two arrays is that we start by comparing the first elements of both arrays, all right? And whichever element is smaller, we put that in the resulting array. So out of two and seven, two is smaller, so we place two in the resulting array and move our pointer to the next element of the left array, because we copy the element from the left array, okay? Then we compare nine and seven, and now, seven is smaller out of the two, so we copy seven to the resulting array, and move the pointer to the next element of the right array. Then we compare nine and eight, and eight being smaller, we copy to the resulting array, and move the pointer. Then we compare nine and 12, and because nine is smaller than 12, we copy nine, and now we are at the end of the first array. So we just copy all the remaining elements of the right array over to the resulting array, and this is how we can merge two individually sorted arrays into a complete sorted array. 

- But this was only the merge step. There are more details for the merge sort algorithm. Let's say we are given an unsorted array of integers. As said earlier, merge sort is a recursive algorithm. 
23,5,2,8,12,7,16,9

23,5,2,8    12,7,16,9

23,5 . 2,8 .   12,7 .  16,9


So we are calling merge sort method on this array, and what we really do is that we break down this array in two logical parts, about midway, and then call merge sort on these two arrays, all right? That is, we try to sort these individual arrays. The idea is that we will sort these two arrays using merge sort, and then merge these two arrays into one single sorted array, all right? So we again call the merge sort algorithm recursively, further breaking down these two arrays into two parts each, and then call a merge sort on these smaller arrays, and we keep doing that until we have broken the array into small arrays of one element each. All right? Now, because they are arrays of single elements, individually, they are already sorted, isn't it? So now, we can start merging them together. So, we merge the first two arrays into a single array, so that the resulting array is sorted. And similarly, we merge the other arrays as well. So now, we have these four arrays, which are individually sorted. So we can merge them together into two arrays, which will themselves be sorted, and finally, we merge the two arrays into a single sorted array, all right? Just think about it for some time. It's not very difficult.


Merge sort: Pseudocode
- [Instructor] Now let's look at the pseudocode for MergeSort. So here we denote the array as capital A, and what we are doing here is that we are saying MergeSort array A from start to end. And what is the start and end? Well, they are indexes of the array representing the part of the array we want to merge sort, all right? So basically we don't really physically break down the array into smaller parts, rather we do a logical breakdown, and start and end indices help us keep tab on which part of the array is being merge sorted, and just to make it clear, MergeSort will be called on array A with the start index as zero and the end index as seven, because the length of the array A is seven, all right? Of course, start is less than end, so we go inside the block of code and calculate the middle, which will be three. Then we call MergeSort recursively on array A, but only on the first part of the array, from index zero to three, all right? Now the next statement will not be executed right away, because the call will again go to the MergeSort method with these arguments. The next statement will only be executed when the MergeSort method returns, which will happen once it reaches the breaking condition, right? So now, MergeSort is called from zeroth index to the third index. Again, start is less than end, so middle is calculated to be one, and then we call MergeSort with zero as the start and one as end, which means that this time we are merge sorting the first two elements. MergeSort called recursively again, middle is zero, and MergeSort again called, but this time with zero and zero as arguments, that is, we just want to merge sort the first element. So now when MergeSort is called with these arguments, the method would return, because there is only one element to be sorted now, and one-element array is already sorted, so we don't do anything, and because this method returns, the next statement calling MergeSort will be executed on array A with arguments start and end as one each. That is, we want to merge sort this element of the array, and here too, start is equal to end, hence the MergeSort method will return without doing anything. So now we will call the Merge method, which will merge these elements of array A, right? Because we just said that this can be logically seen as two single-element arrays, and we are merging these two now. We will come to the Merge routine, but for now, just understand what is MergeSort doing. So after the Merge method returns, the data in the array A will look like this: 23 and five should have exchanged places, and now we will come to the stack frame where a call to MergeSort(A, 0, 3) was made, and now MergeSort(A, 0, 1) has returned after merging the first two elements. Now call to MergeSort(A, 2, 3) is made, which means that this part of the array will be sorted now, all right? So MergeSort will be called recursively, just like we saw in the previous example, and when it returns, the Merge method with zero, one, and three as parameters will be called, which will merge these two individually-sorted parts of the array, so that after this Merge call returns, the array will look like this. This was quite intense. I hope you have understood what's going on, or you can try putting the call stack by writing the sequence of calls on a piece of paper. And now we will get to the Merge routine.
start = 0;
end = A.length -1
MergeSort(A, start, end):
  if start < end :
    middle = Floor [(start + end)/2]
    Mergesort(A, start, middle)
    Mergesort(A, middle+1, end)
    Merge(A, start, middle, end)
    
Merge step: Pseudocode
- [Instructor] Let's get to the pseudocode of the merge method. This is quite simple. Just keep in mind that we have divided the arrays logically. One part of the array is from start to mid index, while the other part of the array is from mid plus one index to the end index, all right? So let's say we are in this last step of merge, when these two parts of the array have been recursively sorted by merge sort, and now we are doing the final merge step. While merging, if we overwrite on the array, we will lose the data, right? So the first thing we can do to keep things simple is we create two temporary arrays and copy the elements from this main array over to those two. So let's first find the size of the arrays. For the left array, the size may be found as mid minus start plus one, and for the second array, the size will be end minus mid, and you can see both parts contain four elements each, so that should be the size of the two arrays. So let's create two arrays of sizes n1 and n2, respectively, and name them left and right. Then we copy the left part of the main array into the left array, and the right part of the main array into the right array, and the pseudocode for these two steps is very simple and straightforward. In these two parts, we are just doing that: copying data from the main array over to the left and right arrays. Now we need to start going through all the elements of the left and right array, so let's declare two variables, i and j, and initialize them to zero. These will act as indexes, or pointers, for the left and right arrays, respectively. By the way, the i and j variables that we used in the for-loops are different, all right? They are local to the for-loop, and their scope ends as soon as the for-loop exits. Then we write another loop to go over the indices of the main array, because now we will be merging the left and right arrays into the main array A, all right? And to do that, we first compare the elements at ith index of the left array, and jth index of the right array. If the ith element in the left array is the smaller, we assign it to the kth element of the main array, and then increment i, that is, go to the next element of the left array. Otherwise, we assign the jth element of the right array to the kth element of the main array, and increment the value of j. And we can keep doing this for all the indices of the main array, all right? So in this example, we first compare two and seven, and two is the smaller, so we copy it over to the kth element of the main array and increment the value of i. Because k is used in a for-loop, its value will also be increased by one in each iteration, so now k moves to the first index. Then we again compare ith element of the left and jth element of right, and five is smaller than seven, hence we assign it to the kth index of A, all right? And we increment the value of i. Now we compare eight and seven, and seven is smaller, hence we place seven at the kth index of A, and now we increment j to move to the next index, and we keep doing this until we reach the end of the arrays. So now, both the left and right arrays are merged into the array A such that A is completely sorted now. By the way, here we have shown the pseudocode. When you write the real Java code, make sure that you check for boundary values of i and j while accessing elements from the left and right arrays, otherwise you may get array out of bound exception, all right? I will attach the java implementation for it right here in this lecture, but make sure that you try to implement it before looking at the solution.

Time complexity of merge sort
- Now let's see how much time does the Merge Sort Algorithm take. Rather is it good or bad compared to the other sorting algorithms that we learned earlier like Insertion Sort. So how do we get an estimate of time? In this algorithm the array is logically broken down into smaller parts, right? So let's say that this is the height of the top recursion. And we can say that each of the breaking up steps creates a layer kind of, okay? So we will generate "H" layers from the full recursion, alright? Now in each of the horizontal layers how many comparisons are made? Well there are N elements entirely hence we will end up making "N" comparisons. And with all the copying of elements in each layer the number of operations will be proportional to "N" And because there are "H" layers hence the total number of operations must be some function of "H" times "N", right? So all we need to do now is find the height or depth of recursion. So let's do that now. Let's see how many logical arrays do we have in each layer? In layer one, how many arrays do you see? One, right? Because this is the start of recursion and we have not really broken down the array at this point. Then in level two, we have two arrays. Which is two arrays to the power of one. Then level three has four arrays or two arrays to the power of two. And how many arrays does the last layer or layer "H" have? It should have two arrays to the power "H" minus one arrays. That is if we just follow the power of 2 pattern. And how many elements are there in the array? Well in general we have "N" elements right? So this number of arrays should be equal to "N." Because we keep breaking down to the point where we have single element arrays. So we can say that two arrays to the power of "H" minus 1 should be equal to "N". Or "H" is equal to one plus log base two of "N". So the time complexity is "N" times "H" or it is of the order of "N" log base two of "N". So Merge Sort is a really fast algorithm if we look at the time complexity. But in reality it is not great for a small number of items because the constant factors are too large. But if you want to sort larger sets of data, this algorithm is to be used. And actually, the Java collections screen work uses Merge Sort as its default sorting algorithm.


The tree data structure
- [Instructor] In this chapter, we will learn binary trees, and more specifically, binary search trees. Binary search tree is a very versatile data structure. That is, it is really fast to insert items into it, it's fast to delete items from it, and it's pretty fast to search items in a binary search tree. 


- If you compare sorted arrays and linked list data structures, we see that search is fast in a sorted array, but quite slow in a linked list. Inserting an item is slow in a sorted array, while it is very fast in case of a linked list that is if we insert the item at the head, alright? Then deleting an item is also slow, if we used a sorted array to sort items, while it is fast, if we used a linked list. Binary search trees are very balanced in the sense that all these common operations like insert, delete, and search take about the same time, which is of the order of log n, alright? And we will see how to arrive at that slightly later in the chapter.
OPEARATION	SORTED ARRAYS	VS	LINKED LIST
Search		Fast O(logn)		Slow O(n)
Insert		Slow O(n)		Fast O(1)
Delete		Slow O(n)		Fast O(1)

Trees:
- A tree data structure may be visualized to be something like this, where we have the nodes, which contain the data.
					A
				/	|	\
				B	C	D
			/	      /	  \		\
			E	     F     G		E
- the nodes are connected to other nodes through what are called as the edges. And it's just like we connected the nodes of a link list through object references
- The node at the top of the tree is called the root node. And in a tree data structure, there can only be one root node. So, just as the link list data structure had a reference to the head node, a tree data structure has a reference to the root node. And all the other nodes can be accessed through that. 
- There is one more property of the tree data structure, and that is to search any node of the tree, there must be only one path from the root node, For example, if these two nodes are connected, then to search the F node, we can start from the root node A, go to C and then F, or we start from A, then go to C, then G and then F. So there are multiple paths from the root node to reach the F node. So that is not allowed in a tree data structure
					A
				/	|	\
				B	C	D
			/	      /	  \		\
			E	     F-----G		E
- Any node other than the root has exactly one edge running upwards to another node. The node above is called the parent of the node. For example, the F node has exactly one edge running upwards to the C node. So, the C node is set to be the parent of F. Actually, C is the parent of both F and G nodes. So, F and G nodes are called the children of the C node. Then a node which has no children is called a leaf node. Any node may be considered to be the root of a subtree, which consists of its children and its children's children and so on. The level of a node tells us how far it is from the root. For example, the root element is assumed to be at level zero. Children of root are set to be at level one. Then their children in turn are set to be at level two and so on.


Binary trees
- A binary tree is one in which every node can have, at most, two children. That is, a node may have no children, which makes them a leaf node. Other node may have only one child, or it may have two children, but none of the nodes can have more than that. That's why they are called binary trees, because binary means two. 
- Now, how do we represent a node in a binary tree? Well, a node contains some data, just like we held data in the node of a linked list, and it also have references to it. Left and right child, which are themselves, tree node type of references, all right? So we can call them left and right, or left child and right child. And of course, the child reference may point to some tree node type of an object. Let's implement some code in Java. 
-In the IDE, let's first create a package for this chapter. And then create a class named TreeNode. Let it contain integer type of data, or if you are aware of generics in Java, you can parameterize this data, but here, I'll focus more on the data structure and (mumbles). Then we can write other two member variables representing the leftChild and rightChild, both being of type TreeNode itself. Let's try the getter and setter methods for leftChild and rightChild. And only the getter for the data. Because once we set the data in the node, there is no need to change it, and we will be setting the data through the constructor. As a good programming practice, immutable objects are good things. So here, the node object is not completely immutable, because we may need to change it appearances to left and rightChild. But we should at least keep the data from changing once it has been set in a node. So we don't provide the setter method for the data member. Now we can write a constructor, which takes the integer data as parameter. So that we can easily create node objects from this data. 





-Now we create another class representing the binary tree object. As said earlier, the binary tree itself contains the reference only to the root node. So let's create a member variable named root, which is of type TreeNode, right? And then, we can write methods like insert and integer object in this tree. Or find an integer object in this tree. And here we have written the TreeNode because we just want to return the node containing the integer data, all right? Or we can delete an integer from this tree object. And that's it. We have pretty much implemented a binary tree, and of course, we need to implement these methods that we wrote, so let's do that in the next lecture.






Binary search trees
- [Instructor] As said earlier, the node in a tree contains the data. For example, the tree shown here contains integer data, but it may also have other kinds of data, like user data, or any other types of object, but let's keep it to integers in our discussion. Now, if the data is randomly placed within the nodes of the tree, it is not very useful, alight? At least for searching data or inserting data et cetera which are the typical operations we do with a tree data structure. So mostly, we don't really use a binary tree data structure as such, rather, we use another variation of it which is called a binary search tree, alright? So what is a binary search tree? Well, a binary search tree is a binary tree with a special property that for any node, all the children to the left, which means it's immediate left child and then the grandchildren and so on, have a data value less than that of the parent node itself, alright? So, let's consider the root element. All it's children on the left, have data values less than the root, alight? And all the children to the right of the node have data values greater than that of the node. And this is true for every node. That is, if you pick any node, this property will be true. Let's take another example. If you consider this node, which has a value of 33, all of these elements to the left have values less than 33, while on the right side, all nodes have a value greater than 33. And keeping this property in a binary tree makes it quite fast to search items, alright? So, just keep in mind left child and the children have values less than the parent, while the right child and its children have values more than the parent node. And this is what a binary search tree is.


Inserting an item in a binary search tree
- [Instructor] We saw how to search for an item in a binary search tree. Now, let's see how to insert a new data item in it. When inserting a new node, we essentially keep looking for a place where to insert this new node such that the binary search tree property is not violated. That is the right child should be greater than or equal to the current node, and the left child should be less than the current node. And, well, we can do the equality thing for the right child in case duplicate values are allowed. So, if the data is equal to the current node, we insert the new value as a right child. But that may not be the immediate right child, alright? We do the insertion recursively. Let's see what I mean. Let's say we have to insert 63 in the binary search tree shown here. We start at the root and compare the two values. So in this case, 63 is greater than 52. So we know that it must be inserted somewhere in the right subtree. So we move to the right child of this root node. Again, we compare the two values. Since 63 is smaller than 65, we know that it must be inserted somewhere in the left subtree from this node. So we move to the left, and now, compare these two values. Now, 63 is greater than 60, right? So, we will need to insert it somewhere in the right subtree. But this node does not have a right child, which means that 63 must be inserted as the right child of this node, doesn't it? And this becomes the breaking condition for recursion. Think about it. Let's implement the insert method now. Let's go to the TreeNode class and write the insert method. And what we can do is that we check, if the integer data to be inserted is greater than or equal to the current node's data, we do something, which we'll come back to in a moment. Otherwise we do something else, which is inserting the nodes somewhere in the left subtree. So if the data to be inserted is greater than or equal to the current node's data, what do we do? We say that if it does not have a right child, this data to be inserted will become the right child of the current node, isn't it? So, let's do that. Otherwise we can recursively insert the data into the right child node, alright? So, again, you can see that recursion makes things quite simple. At least we don't need to write lengthy code. It's quite crisp and elegant code, and we do a similar thing for the other case. But instead of the right child, we check for the left child, alright? And then in the BinarySearchTree class, we write the insert method. And check if the root is null, then the current data should become the root, right? Otherwise we call the insert on the root, so that the data is inserted at the right place using the recursive method that we just wrote.










   
 
  
  































